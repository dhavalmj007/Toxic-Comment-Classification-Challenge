{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "## import packages\n",
    "########################################\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "#from keras import initializations\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '/home/paperspace/Desktop/Kaggle/Embeddings/glove.840B.300d.txt'\n",
    "path = '/home/paperspace/Desktop/Kaggle/Toxic Comment Classification Challenge/Toxic-Comment-Classification-Challenge/'\n",
    "TRAIN_DATA_FILE=path+'train.csv'\n",
    "TEST_DATA_FILE=path+'test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300\n",
    "num_dense = 256\n",
    "rate_drop_lstm = 0.25\n",
    "rate_drop_dense = 0.25\n",
    "\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.48 Âµs\n",
      "Indexing word vectors\n",
      "Total 2195892 word vectors.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## index word vectors\n",
    "########################################\n",
    "%time\n",
    "print('Indexing word vectors')\n",
    "\n",
    "#Glove Vectors\n",
    "embeddings_index = {}\n",
    "counter = 0\n",
    "f = open(EMBEDDING_FILE)\n",
    "for line in f:\n",
    "    #line = line.encode('ascii', 'replace')\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    #print(word)\n",
    "#    counter = counter +1\n",
    "    #print(values[1:])\n",
    "    #if counter==2:\n",
    "        #break\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "print('Processing text dataset')\n",
    "\n",
    "#Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "\n",
    "#regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    #Remove Special Characters\n",
    "    text=special_character_removal.sub('',text)\n",
    "    \n",
    "    #Replace Numbers\n",
    "    text=replace_numbers.sub('n',text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_train = train_df[\"comment_text\"].fillna(\"NA\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_df[list_classes].values\n",
    "list_sentences_test = test_df[\"comment_text\"].fillna(\"NA\").values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))\n",
    "    \n",
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 392183 unique tokens\n",
      "Shape of data tensor: (159571, 150)\n",
      "Shape of label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 25722\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## prepare embeddings\n",
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143613, 150) (143613, 6)\n",
      "(15958, 150) (15958, 6)\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## sample train/validation data\n",
    "########################################\n",
    "np.random.seed(1234)\n",
    "perm = np.random.permutation(len(data))\n",
    "idx_train = perm[:int(len(data)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_train=data[idx_train]\n",
    "labels_train=y[idx_train]\n",
    "print(data_train.shape,labels_train.shape)\n",
    "\n",
    "data_val=data[idx_val]\n",
    "labels_val=y[idx_val]\n",
    "\n",
    "print(data_val.shape,labels_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 300)          30000000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 150, 300)          721200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 300)          0         \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      (None, 300)               450       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               77056     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 30,801,272\n",
      "Trainable params: 800,760\n",
      "Non-trainable params: 30,000,512\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "## define the model structure\n",
    "########################################\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences=True)\n",
    "\n",
    "comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences= embedding_layer(comment_input)\n",
    "x = lstm_layer(embedded_sequences)\n",
    "x = Dropout(rate_drop_dense)(x)\n",
    "merged = Attention(MAX_SEQUENCE_LENGTH)(x)\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "preds = Dense(6, activation='sigmoid')(merged)\n",
    "\n",
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "model = Model(inputs=[comment_input], \\\n",
    "        outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='rmsprop',\n",
    "        metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('simple_lstm_glove_vectors_0.25_0.25.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_lstm_glove_vectors_0.25_0.25\n"
     ]
    }
   ],
   "source": [
    "STAMP = 'simple_lstm_glove_vectors_%.2f_%.2f'%(rate_drop_lstm,rate_drop_dense)\n",
    "print(STAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.3585 - acc: 0.9145\n",
      " ROC-AUC - epoch: 1 - score: 0.871447 \n",
      "\n",
      "143613/143613 [==============================] - 133s 923us/step - loss: 0.3565 - acc: 0.9150 - val_loss: 0.1543 - val_acc: 0.9654\n",
      "Epoch 2/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0752 - acc: 0.9798\n",
      " ROC-AUC - epoch: 2 - score: 0.966269 \n",
      "\n",
      "143613/143613 [==============================] - 126s 879us/step - loss: 0.0752 - acc: 0.9797 - val_loss: 0.1528 - val_acc: 0.9632\n",
      "Epoch 3/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9809\n",
      " ROC-AUC - epoch: 3 - score: 0.977001 \n",
      "\n",
      "143613/143613 [==============================] - 127s 887us/step - loss: 0.0544 - acc: 0.9810 - val_loss: 0.1510 - val_acc: 0.9633\n",
      "Epoch 4/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9815\n",
      " ROC-AUC - epoch: 4 - score: 0.979314 \n",
      "\n",
      "143613/143613 [==============================] - 126s 878us/step - loss: 0.0511 - acc: 0.9815 - val_loss: 0.1329 - val_acc: 0.9636\n",
      "Epoch 5/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9821\n",
      " ROC-AUC - epoch: 5 - score: 0.979971 \n",
      "\n",
      "143613/143613 [==============================] - 126s 879us/step - loss: 0.0488 - acc: 0.9821 - val_loss: 0.1067 - val_acc: 0.9651\n",
      "Epoch 6/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9826\n",
      " ROC-AUC - epoch: 6 - score: 0.981148 \n",
      "\n",
      "143613/143613 [==============================] - 127s 882us/step - loss: 0.0469 - acc: 0.9826 - val_loss: 0.0655 - val_acc: 0.9737\n",
      "Epoch 7/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9828\n",
      " ROC-AUC - epoch: 7 - score: 0.984161 \n",
      "\n",
      "143613/143613 [==============================] - 127s 882us/step - loss: 0.0458 - acc: 0.9828 - val_loss: 0.0528 - val_acc: 0.9794\n",
      "Epoch 8/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9833\n",
      " ROC-AUC - epoch: 8 - score: 0.984801 \n",
      "\n",
      "143613/143613 [==============================] - 126s 876us/step - loss: 0.0446 - acc: 0.9833 - val_loss: 0.0488 - val_acc: 0.9812\n",
      "Epoch 9/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9835\n",
      " ROC-AUC - epoch: 9 - score: 0.984291 \n",
      "\n",
      "143613/143613 [==============================] - 125s 874us/step - loss: 0.0437 - acc: 0.9835 - val_loss: 0.0450 - val_acc: 0.9829\n",
      "Epoch 10/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9838\n",
      " ROC-AUC - epoch: 10 - score: 0.985928 \n",
      "\n",
      "143613/143613 [==============================] - 126s 877us/step - loss: 0.0430 - acc: 0.9838 - val_loss: 0.0443 - val_acc: 0.9828\n",
      "Epoch 11/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9840\n",
      " ROC-AUC - epoch: 11 - score: 0.983858 \n",
      "\n",
      "143613/143613 [==============================] - 126s 878us/step - loss: 0.0421 - acc: 0.9840 - val_loss: 0.0450 - val_acc: 0.9826\n",
      "Epoch 12/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9841\n",
      " ROC-AUC - epoch: 12 - score: 0.985109 \n",
      "\n",
      "143613/143613 [==============================] - 126s 878us/step - loss: 0.0417 - acc: 0.9841 - val_loss: 0.0438 - val_acc: 0.9833\n",
      "Epoch 13/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9842\n",
      " ROC-AUC - epoch: 13 - score: 0.985793 \n",
      "\n",
      "143613/143613 [==============================] - 126s 875us/step - loss: 0.0410 - acc: 0.9842 - val_loss: 0.0440 - val_acc: 0.9832\n",
      "Epoch 14/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9845\n",
      " ROC-AUC - epoch: 14 - score: 0.985148 \n",
      "\n",
      "143613/143613 [==============================] - 126s 878us/step - loss: 0.0403 - acc: 0.9845 - val_loss: 0.0450 - val_acc: 0.9826\n",
      "Epoch 15/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9846\n",
      " ROC-AUC - epoch: 15 - score: 0.986006 \n",
      "\n",
      "143613/143613 [==============================] - 125s 868us/step - loss: 0.0401 - acc: 0.9846 - val_loss: 0.0438 - val_acc: 0.9835\n",
      "Epoch 16/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9848\n",
      " ROC-AUC - epoch: 16 - score: 0.983990 \n",
      "\n",
      "143613/143613 [==============================] - 125s 871us/step - loss: 0.0394 - acc: 0.9847 - val_loss: 0.0433 - val_acc: 0.9831\n",
      "Epoch 17/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9849\n",
      " ROC-AUC - epoch: 17 - score: 0.986538 \n",
      "\n",
      "143613/143613 [==============================] - 126s 876us/step - loss: 0.0388 - acc: 0.9849 - val_loss: 0.0438 - val_acc: 0.9831\n",
      "Epoch 18/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9852\n",
      " ROC-AUC - epoch: 18 - score: 0.986059 \n",
      "\n",
      "143613/143613 [==============================] - 126s 879us/step - loss: 0.0382 - acc: 0.9852 - val_loss: 0.0429 - val_acc: 0.9836\n",
      "Epoch 19/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9853\n",
      " ROC-AUC - epoch: 19 - score: 0.986041 \n",
      "\n",
      "143613/143613 [==============================] - 126s 876us/step - loss: 0.0377 - acc: 0.9853 - val_loss: 0.0442 - val_acc: 0.9838\n",
      "Epoch 20/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9855\n",
      " ROC-AUC - epoch: 20 - score: 0.986082 \n",
      "\n",
      "143613/143613 [==============================] - 126s 877us/step - loss: 0.0372 - acc: 0.9855 - val_loss: 0.0445 - val_acc: 0.9834\n",
      "Epoch 21/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9854\n",
      " ROC-AUC - epoch: 21 - score: 0.986161 \n",
      "\n",
      "143613/143613 [==============================] - 126s 877us/step - loss: 0.0368 - acc: 0.9854 - val_loss: 0.0442 - val_acc: 0.9827\n",
      "Epoch 22/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9858\n",
      " ROC-AUC - epoch: 22 - score: 0.986036 \n",
      "\n",
      "143613/143613 [==============================] - 128s 888us/step - loss: 0.0363 - acc: 0.9858 - val_loss: 0.0440 - val_acc: 0.9836\n",
      "Epoch 23/50\n",
      "142400/143613 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9861\n",
      " ROC-AUC - epoch: 23 - score: 0.986665 \n",
      "\n",
      "143613/143613 [==============================] - 127s 882us/step - loss: 0.0356 - acc: 0.9861 - val_loss: 0.0434 - val_acc: 0.9836\n"
     ]
    }
   ],
   "source": [
    "early_stopping =EarlyStopping(monitor='val_loss', patience=5)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "RocAuc = RocAucEvaluation(validation_data=(data_val, labels_val), interval=1)\n",
    "\n",
    "hist = model.fit(data_train, labels_train, \\\n",
    "        validation_data=(data_val, labels_val), \\\n",
    "        epochs=50, batch_size=1600, shuffle=True, \\\n",
    "         callbacks=[early_stopping, model_checkpoint, RocAuc])\n",
    "         \n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making the submission before fine-tuning\n",
      "153164/153164 [==============================] - 29s 190us/step\n"
     ]
    }
   ],
   "source": [
    "print('Start making the submission before fine-tuning')\n",
    "\n",
    "y_test = model.predict([test_data], batch_size=1024, verbose=1)\n",
    "\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "\n",
    "sample_submission.to_csv('STAMP'+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
